{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dive into Keras source code This is a series of articles explaining the source code of Keras . Learning goals Understand the overall code structures. Understand the mechanisms behind the core workflows, including modeling, training, and saving. Understand the core concepts and classes in Keras and TensorFlow. Understand the important TensorFlow API usages in Keras codebase. Prerequisites You should understand the basic usages of Keras by reading the following two tutorials. Introduction to Keras for Researchers . Introduction to Keras for Engineers .","title":"Introduction"},{"location":"#dive-into-keras-source-code","text":"This is a series of articles explaining the source code of Keras .","title":"Dive into Keras source code"},{"location":"#learning-goals","text":"Understand the overall code structures. Understand the mechanisms behind the core workflows, including modeling, training, and saving. Understand the core concepts and classes in Keras and TensorFlow. Understand the important TensorFlow API usages in Keras codebase.","title":"Learning goals"},{"location":"#prerequisites","text":"You should understand the basic usages of Keras by reading the following two tutorials. Introduction to Keras for Researchers . Introduction to Keras for Engineers .","title":"Prerequisites"},{"location":"01-modeling-api-overview/","text":"The modeling API of Keras is responsible for putting the layers or TensorFlow operations together to create a model before training. A chain of class inheritance Here is a simple Sequential model example, which creates a model with minimal code. import keras model = keras . Sequential () model . add ( keras . layers . Dense ( input_shape = ( 10 ,), units = 10 , activation = 'relu' )) model . add ( keras . layers . Dense ( units = 1 )) The Sequential class is a high-level class, which has a chain of base classes. The inheritance chain looks like this: tf.Module -> Layer -> Model -> Functional -> Sequential The class on the left of an arrow is the base class of the one on the right. To understand this chain of classes, we start from the base classes to see what functionality has been added step-by-step by the subclass down the chain. The tf.Module class ( Source ) The first base class to dive into is the tf.Module class, which is a core class in TensorFlow. It is used by Keras. You can think of it as a container for tf.Variable instances. A tf.Variable instance is a data structure for storing a mutable tensor in TensorFlow. The difference between a tf.Variable and a tf.Tensor is that tf.Variable is mutable but tf.Tensor is not. The weight of a layer is a tf.Variable instance. A typical usage of the tf.Module class is to group a series of operations on the tensors, for example, a neural network layer. It has an attribute called name_scope , which is used as the prefix for the names of its tf.Variable instances. import tensorflow as tf constant_tensor = tf . constant ([ 10 , 20 , 30 ]) class MyModule ( tf . Module ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with self . name_scope : # Open the name space. self . variable = tf . Variable ( constant_tensor , name = \"my_variable\" ) # Create in the name space. # The format of the name is \"`module_name/variable_name:counter`\" print ( MyModule ( name = \"my_module\" ) . variable ) Output: <tf.Variable 'my_module/my_variable:0' shape=(3,) dtype=int32, numpy=array([10, 20, 30], dtype=int32)> However, the name of a tf.Variables is only an internal identifier of the object, which should not be used directly by the users. Because it is not part of the Keras codebase, we will not dive into the implementation of it. Another feature of the class is that it inherits the Trackable class, which tracks all the tf.Variable instances in the attributes of the subclasses of tf.Module . When saving the models, all the tf.Variable instances inside this container can be found and saved. The variables are also tracked for optimizing the computational graph. File locations Before we show how the Layer class works, let's first see where the code of the base Layer class and the subclasses is, like Conv2D . This is a typical case for the organizing code in Keras. The base Layer class is in /keras/engine/base_layer.py , while the subclasses are in the /keras/layers directory. The base classes, which builds the Keras overall framework, are in the /keras/engine directory. The implementation of each of the subclasses is in its corresponding directory. With this file location logic, you can navigate through the codebase. The import mechanism The importing path of the Layer class is tf.keras.layers.Layer . However, the code of the class is in /keras/engine/base_layer.py . It is designed to decouple the importing path and the actual path to give better flexibility for the implementation. This import mechanism is implemented with the @keras_export() decorator, which is implemented using the @tf_export() decorator. With @keras_export('keras.layers.Layer') , the 'Layer' class can be imported from tf.keras.layers.Layer .","title":"1. Modeling API overview"},{"location":"01-modeling-api-overview/#a-chain-of-class-inheritance","text":"Here is a simple Sequential model example, which creates a model with minimal code. import keras model = keras . Sequential () model . add ( keras . layers . Dense ( input_shape = ( 10 ,), units = 10 , activation = 'relu' )) model . add ( keras . layers . Dense ( units = 1 )) The Sequential class is a high-level class, which has a chain of base classes. The inheritance chain looks like this: tf.Module -> Layer -> Model -> Functional -> Sequential The class on the left of an arrow is the base class of the one on the right. To understand this chain of classes, we start from the base classes to see what functionality has been added step-by-step by the subclass down the chain.","title":"A chain of class inheritance"},{"location":"01-modeling-api-overview/#the-tfmodule-class","text":"( Source ) The first base class to dive into is the tf.Module class, which is a core class in TensorFlow. It is used by Keras. You can think of it as a container for tf.Variable instances. A tf.Variable instance is a data structure for storing a mutable tensor in TensorFlow. The difference between a tf.Variable and a tf.Tensor is that tf.Variable is mutable but tf.Tensor is not. The weight of a layer is a tf.Variable instance. A typical usage of the tf.Module class is to group a series of operations on the tensors, for example, a neural network layer. It has an attribute called name_scope , which is used as the prefix for the names of its tf.Variable instances. import tensorflow as tf constant_tensor = tf . constant ([ 10 , 20 , 30 ]) class MyModule ( tf . Module ): def __init__ ( self , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) with self . name_scope : # Open the name space. self . variable = tf . Variable ( constant_tensor , name = \"my_variable\" ) # Create in the name space. # The format of the name is \"`module_name/variable_name:counter`\" print ( MyModule ( name = \"my_module\" ) . variable ) Output: <tf.Variable 'my_module/my_variable:0' shape=(3,) dtype=int32, numpy=array([10, 20, 30], dtype=int32)> However, the name of a tf.Variables is only an internal identifier of the object, which should not be used directly by the users. Because it is not part of the Keras codebase, we will not dive into the implementation of it. Another feature of the class is that it inherits the Trackable class, which tracks all the tf.Variable instances in the attributes of the subclasses of tf.Module . When saving the models, all the tf.Variable instances inside this container can be found and saved. The variables are also tracked for optimizing the computational graph.","title":"The tf.Module class"},{"location":"01-modeling-api-overview/#file-locations","text":"Before we show how the Layer class works, let's first see where the code of the base Layer class and the subclasses is, like Conv2D . This is a typical case for the organizing code in Keras. The base Layer class is in /keras/engine/base_layer.py , while the subclasses are in the /keras/layers directory. The base classes, which builds the Keras overall framework, are in the /keras/engine directory. The implementation of each of the subclasses is in its corresponding directory. With this file location logic, you can navigate through the codebase.","title":"File locations"},{"location":"01-modeling-api-overview/#the-import-mechanism","text":"The importing path of the Layer class is tf.keras.layers.Layer . However, the code of the class is in /keras/engine/base_layer.py . It is designed to decouple the importing path and the actual path to give better flexibility for the implementation. This import mechanism is implemented with the @keras_export() decorator, which is implemented using the @tf_export() decorator. With @keras_export('keras.layers.Layer') , the 'Layer' class can be imported from tf.keras.layers.Layer .","title":"The import mechanism"},{"location":"02-the-layer-class/","text":"( Source ) The Layer class is easy to understand. It is the base class for the neural network layers in Keras. It defines the forward pass of the computation in a layer. Here is a simple example of inheriting the Layer class to build a custom layer. from tensorflow import keras class SimpleDense ( keras . layers . Layer ): def __init__ ( self , units = 32 ): super ( SimpleDense , self ) . __init__ () self . units = units def build ( self , input_shape ): self . w = self . add_weight ( shape = ( input_shape [ - 1 ], self . units ), initializer = 'random_normal' , trainable = True ) self . b = self . add_weight ( shape = ( self . units ,), initializer = 'random_normal' , trainable = True ) def call ( self , inputs ): return tf . matmul ( inputs , self . w ) + self . b From this example, we can see that a Layer instance is a collection of tensors and computations between them. It trackes the tensors in its attributes, and defines the computation in call() . There are 4 methods to look into in the example. They are __init__() , build() , add_weight() , and call() . The __init__() , build() , and call() are expected to be overriden by the users, while add_weight() is not. Let's see how they work one by one. The __init__() function is easy to understand. It just records the arguments from the caller with the attributes. The Layer.build() function ( Source ) The build() function is to create the tf.Variable s in the layer, which are the weight and bias in the example above. Because the tf.Variable s are used by the call() function, it would have to be created before the call() function is called. Moreover, we don't want the variables to be created multiple times. The question we want to answer here is how the build function is called under the hood. A lazy mechanism is implemented for build() with the Layer._maybe_build() function, whose core logic is shown as follows. The Layer instance would use the self.built attribute to record whether build() has been called. Any code that would need the layer to be built would call this _maybe_build() function to ensure the layer is built and built only once. ( Source ) def _maybe_build ( self , inputs ): ... if not self . built : ... input_shapes = tf_utils . get_shapes ( inputs ) ... self . build ( input_shapes ) ... ... The tf_utils.get_shapes(inputs) is a function in Keras to get the shapes of the input tensors. Here is an example of calling _maybe_build() secretly. We create a layer. We call the layer with a tensor without explicitly calling build() . layer = SimpleDense ( 4 ) layer ( tf . ones (( 2 , 2 ))) Output: <tf.Tensor: shape=(2, 4), dtype=float32, numpy= array([[ 0.02684689, -0.07216483, -0.04574138, 0.03925534], [ 0.02684689, -0.07216483, -0.04574138, 0.03925534]], dtype=float32)> The example runs successfully because the layer call would call the __call__() function, which calls the call() function. Before calling the call() function, __call__() would call _maybe_build() first to ensure the tf.Variable s are created. The pseudo-code is shown as follows. ( Source ) class Layer ( module . Module , ... ): def __call__ ( self , inputs , ** kwargs ): ... self . _maybe_build ( inputs ) ... self . call ( inputs ) ... This lazy pattern appears many times in Keras source code. When ensuring something is called and don't want it to be called multiple times, you should use this pattern. The Layer.add_weight() function ( Source ) We would also like to see how these tf.Variable s are created in the add_weight() function. Here is the pseudo-code for the core logic of the add_weight() function. It creates the variable and asks the backend to track the variable. The variable will be appended to different lists depending on if it is trainable. The backend is a file containing some abstractions of the tensorflow functionalities. In many cases, the Keras code would interact with the backend instead of directly calling the TensorFlow APIs. ( Source ) class Layer ( module . Module , ... ): def add_weight ( self , ... ): ... variable = ... # Create the variable. backend . track_variable ( variable ) # Track the variable. if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) The process for creating the variable is a function call , which is not so different from using tf.Variable(...) to directly create the variable. We need the backend to track the variable for model saving and computation optimization. Now, the question is how the backend is tracking the variable. The code is shown as follows. ( Source ) def track_variable ( v ): \"\"\"Tracks the given variable for initialization.\"\"\" if context . executing_eagerly (): return graph = v . graph if hasattr ( v , 'graph' ) else get_graph () _GRAPH_VARIABLES [ graph ] . add ( v ) We encountered two important concepts: eager mode and graph mode . You can click the link for detailed introductions. Here is a short explanation. You can think of eager execution as plain Python code execution. The tensors are all concrete values instead of placeholders. The operations are read and executed only when we run that line of code in the Python interpreter. However, in graph mode, all the tensors and operations are collected in advance to build the computational graph before any actual value is input for computation. The graph is then optimized for execution speed. It is similar to a compiled language, like the C programming language, which you can turn on various optimization options to make the compiled executable file run faster. TensorFlow API tf.executing_eagerly() is to check whether TensorFlow is running in eager mode or not. ( Link ) By default, everything runs in eager mode. As shown in the code above, in eager mode, we don't need to track the variables because it would not compile the computation graph. In graph mode, the code above would record the tf.Variable to the _GRAPH_VARIABLES , which is a dictionary mapping the TensorFlow computational graphs to a list of tf.Variable s. With this dictionary, Keras can track all the tf.Variable s for features like clearing the value of them. The Layer.call() function ( Source ) Let's see another use case of a layer. Instead of being part of a model, the layer can directly be called to get the output. We can call the layer with a Numpy array, it returns a tf.Tensor as the result. import tensorflow as tf import numpy as np layer = tf . keras . layers . Dense ( input_shape = ( 10 ,), units = 15 ) x = np . random . rand ( 20 , 10 ) output = layer ( x ) print ( output . shape ) # (20, 15) When we call layer(x) , it calls the __call__() function of the Layer() class. The __call__() function would call the call() function, which implements the forward pass of the layer. Calling the layer with a Numpy array, the __call__() function will just convert it to a tf.Tensor and call the call() function. If in eager mode, the function are directly called and executed eagerly. If in graph mode, we will need to convert the function to a computational graph before calling it. TensorFlow API tf.function() is the public API in TensorFlow for converting a normal function into a computation graph. ( Link ) There is another use case that quite separated from the use case above, which is calling the layer with a KerasTensor . It is a class used when creating models using the functional API . It is a symbolic tensor without actual value, but only representing the shapes and types of intermediate output tensors between the layers. We will introduce more about it when we introduce the Model class. The pseudo-code for the __call__() function is shown as follows. class Layer ( module . Module , ... ): def __call__ ( self , inputs , ** kwargs ): if isinstance ( inputs , keras_tensor . KerasTensor ): inputs = convert_to_tf_tensor ( inputs ) outputs = self . call ( inputs ) return convert_to_keras_tensor ( outputs ) if isinstance ( inputs , np . ndarray ): inputs = tf . Tensor ( inputs ) # Check the inputs are compatible with the layer. input_spec . assert_input_compatibility ( self . input_spec , inputs , self . name ) if context . executing_eagerly (): return self . call ( inputs ) else : call_fn = convert_to_tf_function ( self . call ) return call_fn ( inputs ) Input compatibility checking Notably, in the code above, the inputs compatibility is being checked before the actual call. This is to ensure the bug is being caught early and throw out a meaningful error message. If you run the following code, you will get an error. It calls the Dense layer with vectors of length 5 first, which causes the layer to initialize the weights for these vectors. Then, it calls the same layer again with vectors of length 4, which are not compatible with the weights created just now. layer = layers . Dense ( 3 ) layer ( np . random . rand ( 10 , 5 )) layer ( np . random . rand ( 10 , 4 )) Error message: ValueError: Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 5 but received input with shape (10, 4) The inputs compatibility information is recorded in self.input_spec of a layer, which is a function with @property decorator. Now, we would like to see how are the input_spec being recorded and used to check new inputs. The base Layer class doesn't record this input_spec . Each layer should record their own self.input_spec in build() . In build() , they usually need to create the weights of the layer using the input_shape . Meanwhile, they can define a self.input_spec for what inputs are compatible with these weights. For example, in Dense.build() , they set the input_spec for a fixed last dimension by creating an InputSpec instance ( Source ). The signature of the InputSpec is as follows. There are many specifications to set, like the shape, data type, number of dimensions, upper and lower bound of dimensions. ( Source ) class InputSpec ( object ): def __init__ ( self , dtype = None , shape = None , ndim = None , max_ndim = None , min_ndim = None , axes = None , allow_last_axis_squeeze = False , name = None ): In __call__() , assert_input_compatibility() would check all the specifications of the recorded self.input_spec for the given inputs for compatibility. ( Source )","title":"2. The Layer class"},{"location":"02-the-layer-class/#the-layerbuild-function","text":"( Source ) The build() function is to create the tf.Variable s in the layer, which are the weight and bias in the example above. Because the tf.Variable s are used by the call() function, it would have to be created before the call() function is called. Moreover, we don't want the variables to be created multiple times. The question we want to answer here is how the build function is called under the hood. A lazy mechanism is implemented for build() with the Layer._maybe_build() function, whose core logic is shown as follows. The Layer instance would use the self.built attribute to record whether build() has been called. Any code that would need the layer to be built would call this _maybe_build() function to ensure the layer is built and built only once. ( Source ) def _maybe_build ( self , inputs ): ... if not self . built : ... input_shapes = tf_utils . get_shapes ( inputs ) ... self . build ( input_shapes ) ... ... The tf_utils.get_shapes(inputs) is a function in Keras to get the shapes of the input tensors. Here is an example of calling _maybe_build() secretly. We create a layer. We call the layer with a tensor without explicitly calling build() . layer = SimpleDense ( 4 ) layer ( tf . ones (( 2 , 2 ))) Output: <tf.Tensor: shape=(2, 4), dtype=float32, numpy= array([[ 0.02684689, -0.07216483, -0.04574138, 0.03925534], [ 0.02684689, -0.07216483, -0.04574138, 0.03925534]], dtype=float32)> The example runs successfully because the layer call would call the __call__() function, which calls the call() function. Before calling the call() function, __call__() would call _maybe_build() first to ensure the tf.Variable s are created. The pseudo-code is shown as follows. ( Source ) class Layer ( module . Module , ... ): def __call__ ( self , inputs , ** kwargs ): ... self . _maybe_build ( inputs ) ... self . call ( inputs ) ... This lazy pattern appears many times in Keras source code. When ensuring something is called and don't want it to be called multiple times, you should use this pattern.","title":"The Layer.build() function"},{"location":"02-the-layer-class/#the-layeradd_weight-function","text":"( Source ) We would also like to see how these tf.Variable s are created in the add_weight() function. Here is the pseudo-code for the core logic of the add_weight() function. It creates the variable and asks the backend to track the variable. The variable will be appended to different lists depending on if it is trainable. The backend is a file containing some abstractions of the tensorflow functionalities. In many cases, the Keras code would interact with the backend instead of directly calling the TensorFlow APIs. ( Source ) class Layer ( module . Module , ... ): def add_weight ( self , ... ): ... variable = ... # Create the variable. backend . track_variable ( variable ) # Track the variable. if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) The process for creating the variable is a function call , which is not so different from using tf.Variable(...) to directly create the variable. We need the backend to track the variable for model saving and computation optimization. Now, the question is how the backend is tracking the variable. The code is shown as follows. ( Source ) def track_variable ( v ): \"\"\"Tracks the given variable for initialization.\"\"\" if context . executing_eagerly (): return graph = v . graph if hasattr ( v , 'graph' ) else get_graph () _GRAPH_VARIABLES [ graph ] . add ( v ) We encountered two important concepts: eager mode and graph mode . You can click the link for detailed introductions. Here is a short explanation. You can think of eager execution as plain Python code execution. The tensors are all concrete values instead of placeholders. The operations are read and executed only when we run that line of code in the Python interpreter. However, in graph mode, all the tensors and operations are collected in advance to build the computational graph before any actual value is input for computation. The graph is then optimized for execution speed. It is similar to a compiled language, like the C programming language, which you can turn on various optimization options to make the compiled executable file run faster. TensorFlow API tf.executing_eagerly() is to check whether TensorFlow is running in eager mode or not. ( Link ) By default, everything runs in eager mode. As shown in the code above, in eager mode, we don't need to track the variables because it would not compile the computation graph. In graph mode, the code above would record the tf.Variable to the _GRAPH_VARIABLES , which is a dictionary mapping the TensorFlow computational graphs to a list of tf.Variable s. With this dictionary, Keras can track all the tf.Variable s for features like clearing the value of them.","title":"The Layer.add_weight() function"},{"location":"02-the-layer-class/#the-layercall-function","text":"( Source ) Let's see another use case of a layer. Instead of being part of a model, the layer can directly be called to get the output. We can call the layer with a Numpy array, it returns a tf.Tensor as the result. import tensorflow as tf import numpy as np layer = tf . keras . layers . Dense ( input_shape = ( 10 ,), units = 15 ) x = np . random . rand ( 20 , 10 ) output = layer ( x ) print ( output . shape ) # (20, 15) When we call layer(x) , it calls the __call__() function of the Layer() class. The __call__() function would call the call() function, which implements the forward pass of the layer. Calling the layer with a Numpy array, the __call__() function will just convert it to a tf.Tensor and call the call() function. If in eager mode, the function are directly called and executed eagerly. If in graph mode, we will need to convert the function to a computational graph before calling it. TensorFlow API tf.function() is the public API in TensorFlow for converting a normal function into a computation graph. ( Link ) There is another use case that quite separated from the use case above, which is calling the layer with a KerasTensor . It is a class used when creating models using the functional API . It is a symbolic tensor without actual value, but only representing the shapes and types of intermediate output tensors between the layers. We will introduce more about it when we introduce the Model class. The pseudo-code for the __call__() function is shown as follows. class Layer ( module . Module , ... ): def __call__ ( self , inputs , ** kwargs ): if isinstance ( inputs , keras_tensor . KerasTensor ): inputs = convert_to_tf_tensor ( inputs ) outputs = self . call ( inputs ) return convert_to_keras_tensor ( outputs ) if isinstance ( inputs , np . ndarray ): inputs = tf . Tensor ( inputs ) # Check the inputs are compatible with the layer. input_spec . assert_input_compatibility ( self . input_spec , inputs , self . name ) if context . executing_eagerly (): return self . call ( inputs ) else : call_fn = convert_to_tf_function ( self . call ) return call_fn ( inputs )","title":"The Layer.call() function"},{"location":"02-the-layer-class/#input-compatibility-checking","text":"Notably, in the code above, the inputs compatibility is being checked before the actual call. This is to ensure the bug is being caught early and throw out a meaningful error message. If you run the following code, you will get an error. It calls the Dense layer with vectors of length 5 first, which causes the layer to initialize the weights for these vectors. Then, it calls the same layer again with vectors of length 4, which are not compatible with the weights created just now. layer = layers . Dense ( 3 ) layer ( np . random . rand ( 10 , 5 )) layer ( np . random . rand ( 10 , 4 )) Error message: ValueError: Input 0 of layer dense is incompatible with the layer: expected axis -1 of input shape to have value 5 but received input with shape (10, 4) The inputs compatibility information is recorded in self.input_spec of a layer, which is a function with @property decorator. Now, we would like to see how are the input_spec being recorded and used to check new inputs. The base Layer class doesn't record this input_spec . Each layer should record their own self.input_spec in build() . In build() , they usually need to create the weights of the layer using the input_shape . Meanwhile, they can define a self.input_spec for what inputs are compatible with these weights. For example, in Dense.build() , they set the input_spec for a fixed last dimension by creating an InputSpec instance ( Source ). The signature of the InputSpec is as follows. There are many specifications to set, like the shape, data type, number of dimensions, upper and lower bound of dimensions. ( Source ) class InputSpec ( object ): def __init__ ( self , dtype = None , shape = None , ndim = None , max_ndim = None , min_ndim = None , axes = None , allow_last_axis_squeeze = False , name = None ): In __call__() , assert_input_compatibility() would check all the specifications of the recorded self.input_spec for the given inputs for compatibility. ( Source )","title":"Input compatibility checking"},{"location":"03-the-model-class/","text":"( Source ) The Model class is a subclass of Layer . For more details of how to subclassing it to implement your own model, you may check out this tutorial . In the following workflow, the Model class is not so different from the Layer class if you see it as a way to group the layers to build a computational graph. class MyModel ( tf . keras . Model ): def __init__ ( self ): super ( MyModel , self ) . __init__ () self . dense1 = tf . keras . layers . Dense ( 4 , activation = tf . nn . relu ) self . dense2 = tf . keras . layers . Dense ( 5 , activation = tf . nn . softmax ) self . dropout = tf . keras . layers . Dropout ( 0.5 ) def call ( self , inputs , training = False ): x = self . dense1 ( inputs ) if training : x = self . dropout ( x , training = training ) return self . dense2 ( x ) However, it adds a set of functions and attributes that related to training, for example. compile() , fit() , evaluate() , predict() , optimizer , loss , metrics , which we would go into more details when we introduce the training APIs. In summary, a Model can be trained by itself, but a Layer cannot.","title":"3. The Model class"},{"location":"04-the-functional-class/","text":"( Source ) There is another way of using the Model class besides subclassing it, which is the functional API. It connects the layers to each other to form a directed acyclic graph (DAG), where the nodes are layer call events, and the edges are KerasTensors. Please refer to this tutorial for more details of how to use it. Following is a code example of using the functional API. inputs = tf . keras . Input ( shape = ( 3 ,)) x = tf . keras . layers . Dense ( 4 , activation = tf . nn . relu )( inputs ) outputs = tf . keras . layers . Dense ( 5 , activation = tf . nn . softmax )( x ) model = tf . keras . Model ( inputs = inputs , outputs = outputs ) Although it looks like it is still using the Model class, it is using the Functional class, which is an internal class not exposed to the public API. In Model.__new__() , it creates a Functional instance if using the functional API. The source code looks like this. ( Source ) class Model ( Layer ): def __new__ ( cls , * args , ** kwargs ): if is_functional_model_init_params ( args , kwargs ) and cls == Model : return functional . Functional ( skip_init = True , * args , ** kwargs ) ... Now, let's see how Functional is tracking these layers and intermediate outputs in the computational graph. The KerasTensor class keras.Input() , which looks like a class, but it is a function, which returns a KerasTensor object. print ( type ( keras . Input ( shape = ( 28 , 28 , 1 )))) Outputs: <class 'keras.engine.keras_tensor.KerasTensor'> KerasTensor is a class just to represent the intermediate output tensors of the layers in a Keras model, which has some useful properties like shape and dtype . ( Source ) class KerasTensor ( object ): @property def shape ( self ): ... @property def dtype ( self ): ... It is passed to each of the layers by calling them as shown in the functional API example. The purpose is for the layers to create the weights using the shape and type information of the input tensor. That is also why we have a special judge to see if the input tensor is a KerasTensor in Layer.__call__() as we introduced before. class Layer ( module . Module , ... ): def __call__ ( self , inputs , ** kwargs ): if isinstance ( inputs , keras_tensor . KerasTensor ): inputs = convert_to_tf_tensor ( inputs ) outputs = self . call ( inputs ) return convert_to_keras_tensor ( outputs ) ... From the source code above, we can see if we call a layer with a KerasTensor , the return value is also a KerasTensor , which will be used to call the next layer. Connecting the layers The question we try to answer here is: How did the computational graph being recorded and fetched only given the inputs and outputs ? This functionality is implemented in Functional._init_graph_network() . The graph is being fetched starting from the outputs , which is a list of KerasTensor s. Each KerasTensor records the Layer instance that produces it during the call of the Layer . The algorithm is like this. First, from the outputs , we got the Layer producing these outputs . Second, use the Layer to get the input KerasTensors . Third, use these KerasTensor s to get the previous layers. Keep doing this until the inputs to the model are reached. Here are another two questions to answer: How does a KerasTensor get the Layer producing it? How does the Layer get the input KerasTensor s? To answer these two questions, there are two important classes or concepts to make clear first: Node and KerasHistory . A Node is created at each call of a layer, to represent the connectivity between the two Layer s. In other words, a Node corresponds to a call of a Layer . Each Layer has an attribute of _inbound_nodes to track the input Node s. The reason why there can be multiple inbound Node s is that a Layer may be used in multiple places in a model for weight sharing. In the following example, layer_a and layer_b are all called multiple times. Therefore, node1 and node4 are in layer_a._inbound_nodes . node2 and node5 are in layer_b._inbound_nodes . node1 -> layer_a -> node2 -> layer_b -> node3 node4 -> layer_a -> node5 -> layer_b -> node6 When building a functional model, we can call a Layer with multiple KerasTensor s. For example, the Add layer add multiple tensors together. Therefore, a call of a layer corresponds to multiple KerasTensor s. A Node also corresponds to a call of a Layer . Therefore, a Node may correspond to multiple KerasTensor s. A Node record these KerasTensor s in Node._keras_inputs . KerasHistory is for the KerasTensor to find the input Layer and the corresponding Node . It is stored in the attribute of KerasTensor._keras_history . KerasHistory.layer records the Layer producing it. KerasHistory.node_index records the index of the corresponding Node in the KerasHistory.layer._inbound_nodes . For example, if node2 has a corresponding KerasTensor , named keras_tensor_2 , keras_tensor_2._keras_history.node_index records the index of node1 in layer_a._inbound_nodes . With all these recording mechanisms, we can have the following pseudo-code. ( Source ) class Layer ( tf . Module ): def __call__ ( inputs ): ... outputs = self . call ( inputs ) node = Node ( self , inputs , outputs ) ... ( Source ) class Node : def __init__ ( self , layer , inputs , outputs ): ... self . _keras_inputs = inputs layer . _inbound_nodes . append ( self ) node_index = len ( self . layer . _inbound_nodes ) - 1 for keras_tensor in outputs : keras_tensor . _keras_history = KerasHistory ( layer , node_index ) ... Now, we have the answers to the two questions above. A KerasTensor find the layer producing it with KerasTensor._keras_history.layer . A Layer find the input KerasTensor with Layer._inbound_nodes[0]._keras_inputs . Finally, we can fetch the entire computational graph with the algorithm described above. The pseudo-code is shown as follows. ( Source ) def fetch_nodes ( output_keras_tensor ): if output_keras_tensor in model . inputs : return [] layer = output_keras_tensor . _keras_history . layer node_index = output_keras_tensor . _keras_history . node_index node = layer . _inbound_nodes [ node_index ] node_list = [ node ] for input_keras_tensor in node . _keras_inputs : node_list += fetch_nodes ( input_keras_tensor ) return node_list The actual code would not only fetch the nodes, but also the layers, and sort them in topological order. In Functional.call , it calls the layer in topological order to produce the outputs.","title":"4. The Functional class"},{"location":"04-the-functional-class/#the-kerastensor-class","text":"keras.Input() , which looks like a class, but it is a function, which returns a KerasTensor object. print ( type ( keras . Input ( shape = ( 28 , 28 , 1 )))) Outputs: <class 'keras.engine.keras_tensor.KerasTensor'> KerasTensor is a class just to represent the intermediate output tensors of the layers in a Keras model, which has some useful properties like shape and dtype . ( Source ) class KerasTensor ( object ): @property def shape ( self ): ... @property def dtype ( self ): ... It is passed to each of the layers by calling them as shown in the functional API example. The purpose is for the layers to create the weights using the shape and type information of the input tensor. That is also why we have a special judge to see if the input tensor is a KerasTensor in Layer.__call__() as we introduced before. class Layer ( module . Module , ... ): def __call__ ( self , inputs , ** kwargs ): if isinstance ( inputs , keras_tensor . KerasTensor ): inputs = convert_to_tf_tensor ( inputs ) outputs = self . call ( inputs ) return convert_to_keras_tensor ( outputs ) ... From the source code above, we can see if we call a layer with a KerasTensor , the return value is also a KerasTensor , which will be used to call the next layer.","title":"The KerasTensor class"},{"location":"04-the-functional-class/#connecting-the-layers","text":"The question we try to answer here is: How did the computational graph being recorded and fetched only given the inputs and outputs ? This functionality is implemented in Functional._init_graph_network() . The graph is being fetched starting from the outputs , which is a list of KerasTensor s. Each KerasTensor records the Layer instance that produces it during the call of the Layer . The algorithm is like this. First, from the outputs , we got the Layer producing these outputs . Second, use the Layer to get the input KerasTensors . Third, use these KerasTensor s to get the previous layers. Keep doing this until the inputs to the model are reached. Here are another two questions to answer: How does a KerasTensor get the Layer producing it? How does the Layer get the input KerasTensor s? To answer these two questions, there are two important classes or concepts to make clear first: Node and KerasHistory . A Node is created at each call of a layer, to represent the connectivity between the two Layer s. In other words, a Node corresponds to a call of a Layer . Each Layer has an attribute of _inbound_nodes to track the input Node s. The reason why there can be multiple inbound Node s is that a Layer may be used in multiple places in a model for weight sharing. In the following example, layer_a and layer_b are all called multiple times. Therefore, node1 and node4 are in layer_a._inbound_nodes . node2 and node5 are in layer_b._inbound_nodes . node1 -> layer_a -> node2 -> layer_b -> node3 node4 -> layer_a -> node5 -> layer_b -> node6 When building a functional model, we can call a Layer with multiple KerasTensor s. For example, the Add layer add multiple tensors together. Therefore, a call of a layer corresponds to multiple KerasTensor s. A Node also corresponds to a call of a Layer . Therefore, a Node may correspond to multiple KerasTensor s. A Node record these KerasTensor s in Node._keras_inputs . KerasHistory is for the KerasTensor to find the input Layer and the corresponding Node . It is stored in the attribute of KerasTensor._keras_history . KerasHistory.layer records the Layer producing it. KerasHistory.node_index records the index of the corresponding Node in the KerasHistory.layer._inbound_nodes . For example, if node2 has a corresponding KerasTensor , named keras_tensor_2 , keras_tensor_2._keras_history.node_index records the index of node1 in layer_a._inbound_nodes . With all these recording mechanisms, we can have the following pseudo-code. ( Source ) class Layer ( tf . Module ): def __call__ ( inputs ): ... outputs = self . call ( inputs ) node = Node ( self , inputs , outputs ) ... ( Source ) class Node : def __init__ ( self , layer , inputs , outputs ): ... self . _keras_inputs = inputs layer . _inbound_nodes . append ( self ) node_index = len ( self . layer . _inbound_nodes ) - 1 for keras_tensor in outputs : keras_tensor . _keras_history = KerasHistory ( layer , node_index ) ... Now, we have the answers to the two questions above. A KerasTensor find the layer producing it with KerasTensor._keras_history.layer . A Layer find the input KerasTensor with Layer._inbound_nodes[0]._keras_inputs . Finally, we can fetch the entire computational graph with the algorithm described above. The pseudo-code is shown as follows. ( Source ) def fetch_nodes ( output_keras_tensor ): if output_keras_tensor in model . inputs : return [] layer = output_keras_tensor . _keras_history . layer node_index = output_keras_tensor . _keras_history . node_index node = layer . _inbound_nodes [ node_index ] node_list = [ node ] for input_keras_tensor in node . _keras_inputs : node_list += fetch_nodes ( input_keras_tensor ) return node_list The actual code would not only fetch the nodes, but also the layers, and sort them in topological order. In Functional.call , it calls the layer in topological order to produce the outputs.","title":"Connecting the layers"},{"location":"05-the-sequential-class/","text":"( Source ) The Sequential class extends the Functional class. It mainly supports a special case of the Functional model, where only a single chain of layers in the model without any branches. For more details of how to use it, you can check out this tutorial . It implements the add() method and the pop() method to easily handle adding an removing the layers. Sequential has two ways to build the model depending whether the input_shape of the model is know from the beginning. In the following example, the model knows the input_shape from the beginning. It just treats the model as a Functional model. model = keras . Sequential () model . add ( keras . Input ( shape = ( 10 ,))) model . add ( keras . layers . Dense ( units = 10 , activation = 'relu' )) model . add ( keras . layers . Dense ( units = 1 )) However, in the following example, the model would not know the input_shape until it sees the first batch of training data. Therefore, the initialization of the computational graph is deferred. model = keras . Sequential () model . add ( keras . layers . Dense ( units = 10 , activation = 'relu' )) model . add ( keras . layers . Dense ( units = 1 )) The pseudo-code for checking the two cases is as follows. class Sequential ( Functional ): def add ( self , layer ): ... if self . _has_input_shape : # This is the funciton used by `Functional` # to build the computational graph. self . _init_graph_network ( self . inputs , self . outputs ) else : self . layers . append ( layer ) ... def call ( self , inputs , ... ): if not self . _has_input_shape : self . _build_graph_network ( inputs . shape ) ... Summary So far, we have gone through the framework of all the code for model building. We have introduced the chain of extension, from tf.Module to Sequential , what functionalities are added in each subclass along the way. We also introduced some important concepts, like eager mode, graph mode, Tensor , Variable , KerasTensor , and Node . We also introduced some important mechanisms, like the @keras_export , _maybe_build() to ensure the model is only being built for once, creating and tracking the weights, InputSpec checking, computational graph fetching in Functional . Next, we will introduce the source code of the training APIs of Keras. We will see how does Model.compile() and Model.fit() works, how the loss are being tracked, how the optimizer updates the weights, and so on.","title":"5. The Sequential class"},{"location":"05-the-sequential-class/#summary","text":"So far, we have gone through the framework of all the code for model building. We have introduced the chain of extension, from tf.Module to Sequential , what functionalities are added in each subclass along the way. We also introduced some important concepts, like eager mode, graph mode, Tensor , Variable , KerasTensor , and Node . We also introduced some important mechanisms, like the @keras_export , _maybe_build() to ensure the model is only being built for once, creating and tracking the weights, InputSpec checking, computational graph fetching in Functional . Next, we will introduce the source code of the training APIs of Keras. We will see how does Model.compile() and Model.fit() works, how the loss are being tracked, how the optimizer updates the weights, and so on.","title":"Summary"},{"location":"06-training-api-overview/","text":"From here on, we will start to introduce how the training process works. We will try to understand what happends behind the scene of the following code. model . compile ( optimizer = \"adam\" , loss = \"mse\" , metrics = [ \"mae\" ] ) model . fit ( x = np . random . rand ( 100 , 10 ), y = np . random . rand ( 100 , 1 ), epochs = 2 ) We will introduce the following items: * How the Model.compile() function works. * How the Model.fit() , Model.predict() , and Model.evaluate() function works. * How the TensorFlow distributed training API is used in training. * How are the optimizer, loss, and metrics implemented and used.","title":"6. Training API overview"},{"location":"07-the-compile-function/","text":"( Source ) We usually need to call Model.compile() before we train the model, as shown in the following code example. model . compile ( optimizer = \"adam\" , loss = \"mse\" , metrics = [ \"mae\" ] ) As you expected, the Model.compile() function is just recording these configurations. Since the user may provide the optimizer as a string, we use get_optimizer() function to get the corresponding keras.optimizers.Optimizer instance. The loss and metrics can be lists or dictionaries of loss functions and metrics. Threrefore, we need to encapsulate them into data structures, which are easier to use, which can be treated as single objects instead of using a for loops to deal with each of the losses or metrics. The core functionality of Model.compile() is shown as in the following pseudo code. ( Source ) class Model ( Layer ): def compile ( self , loss , optimizer , metrics , ... ): self . optimizer = get_optimizer ( optimizer ) self . compiled_loss = LossesContainer ( loss ) self . compiled_metrics = MetricsContainer ( metrics ) Besides the loss, optimizer, and metrics, which are the most important configurations for the training, there are other interesting configurations in teh compile function as well, you may try to explore them in the source code by yourself.","title":"7. The compile function"},{"location":"08-the-fit-function/","text":"( Source ) The first thing the Model.fit() function does is to convert the user passed dataset ( x and y ) into a compatible format that ready to be used for the training. They are wrapped into a DataHandler , which can be used conveniently during the training. It has a method, named enumerate_epochs() , which returns the current epoch number and the iterator of the dataset. It also has a method, named steps() , which returns the current step number. These two functions are mainly used by the Model.fit() function to iterate over the dataset for each epoch. In side DataHandler , it would convert different types of data into a tf.data.Dataset object with different DataAdapter s. That is how Keras supports so many different types of data inputs. With DataHandler , we prepared the dataset to be iterated batch-by-batch. For each batch of data, we would do a forward pass and updates of the weights, which is called a step in an epoch. We want to build a function to execute a single step, and compile it into a tf.function to accelerate the process. Here we use Model.make_train_function() to get the function. Now we can use the following pseudo code to summarize what happend in Model.fit() . We first wrap the data into a DataHandler . Get the tf.function for running a step. Use a for loop to iterate through the epochs, and use an inner for loop to iterate the steps. In each step, we just call the function to execute the step. class Model ( Layer ): def fit ( self , x , y , ... ): data_handler = DataHandler ( x , y ) self . train_function = self . make_train_function () for epoch , iterator in data_handler . enumerate_epochs (): for step in data_handler . steps (): self . train_function ( iterator ) Train step Now, it comes to the question of what happens in Model.make_train_function() . You can think it just returns the Model.train_step() ). Notebly, the user can also override this Model.train_step() function to customize their training step . Following is the pseudo code for Model.train_step() . It runs the forward pass using self(x) and compute the loss value, while recording all the gradients with tf.GradientTape() . Then, it use the optimizer to minimize the loss function to update the trainable variables using the gradients. Finally, it returns the metrics. class Model ( Layer ): def train_step ( self , data ): x , y = data_adapter . unpack ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) loss = self . compiled_loss ( y , y_pred ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . compiled_metrics . update_state ( y , y_pred ) return { metric . name : metric . result () for metric in self . metrics } Distributed training Actually, Model.make_train_function() adds one more functionality to Model.train_step() , which is to support the distributed training. Let's see how distributed training is supported in Keras with the TensorFlow APIs. First, we need to use tf.distribute.Strategy.scope() , which opens up a scope to track all the TensorFlow variables created in this scope, for example the weights of the neural network. TensorFlow API tf.distribute.Strategy.scope() . scope() opens up a scope that any tf.Variable() created inside the scope is caught by TensorFlow to run distributedly. To ensure everything is caught by the distributed strategy, we need to put almost the entire Model.fit() function in the scope as shown in the following pseudo code. class Model ( Layer ): def fit ( self , x , y , ... ): with self . distribute_strategy . scope (): data_handler = data_adapter . get_data_handler ( x , y ) self . train_function = self . make_train_function () for epoch , iterator in data_handler . enumerate_epochs (): for step in data_handler . steps (): self . train_function ( iterator ) Another TensorFlow API we need to use here is tf.distribute.Strategy.run() . When run distributedly, the Model.train_step() function needs to run on each replica in parallel. The Model.make_train_function() function wraps Model.train_step() into another function that uses tf.distribute.Strategy.run() to run train_step() distributedly. It also convert the function into a tf.function . TensorFlow API tf.distribute.Strategy.run() runs a function on each replica with the given arguments. For example, strategy.run(fn, args=(arg1, arg2)) . The pseudo code of wrapping Model.train_step() is as follows. We wrap .train_step() into a new function, train_function() , convert it to a tf.function , and return it to .fit() . In train_function() , we just call .train_step() using distribute.Strategy.run() and aggregate the outputs and return it. class Model ( Layer ): def make_train_function ( self , ... ): def train_function ( iterator ): data = next ( iterator ) outputs = model . distribute_strategy . run ( self . train_step , args = ( data ,)) outputs = reduce_per_replica ( outputs ) return outputs train_function = tf . function ( train_function ) return train_function There is another TensorFlow distribute strategy API that is used by Keras is tf.distribute.get_strategy() . That is how Keras get the distribute strategy defined by the user. TensorFlow API tf.distribute.get_strategy() Returns the current tf.distribute.Strategy object.","title":"8. The fit function"},{"location":"08-the-fit-function/#train-step","text":"Now, it comes to the question of what happens in Model.make_train_function() . You can think it just returns the Model.train_step() ). Notebly, the user can also override this Model.train_step() function to customize their training step . Following is the pseudo code for Model.train_step() . It runs the forward pass using self(x) and compute the loss value, while recording all the gradients with tf.GradientTape() . Then, it use the optimizer to minimize the loss function to update the trainable variables using the gradients. Finally, it returns the metrics. class Model ( Layer ): def train_step ( self , data ): x , y = data_adapter . unpack ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) loss = self . compiled_loss ( y , y_pred ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . compiled_metrics . update_state ( y , y_pred ) return { metric . name : metric . result () for metric in self . metrics }","title":"Train step"},{"location":"08-the-fit-function/#distributed-training","text":"Actually, Model.make_train_function() adds one more functionality to Model.train_step() , which is to support the distributed training. Let's see how distributed training is supported in Keras with the TensorFlow APIs. First, we need to use tf.distribute.Strategy.scope() , which opens up a scope to track all the TensorFlow variables created in this scope, for example the weights of the neural network. TensorFlow API tf.distribute.Strategy.scope() . scope() opens up a scope that any tf.Variable() created inside the scope is caught by TensorFlow to run distributedly. To ensure everything is caught by the distributed strategy, we need to put almost the entire Model.fit() function in the scope as shown in the following pseudo code. class Model ( Layer ): def fit ( self , x , y , ... ): with self . distribute_strategy . scope (): data_handler = data_adapter . get_data_handler ( x , y ) self . train_function = self . make_train_function () for epoch , iterator in data_handler . enumerate_epochs (): for step in data_handler . steps (): self . train_function ( iterator ) Another TensorFlow API we need to use here is tf.distribute.Strategy.run() . When run distributedly, the Model.train_step() function needs to run on each replica in parallel. The Model.make_train_function() function wraps Model.train_step() into another function that uses tf.distribute.Strategy.run() to run train_step() distributedly. It also convert the function into a tf.function . TensorFlow API tf.distribute.Strategy.run() runs a function on each replica with the given arguments. For example, strategy.run(fn, args=(arg1, arg2)) . The pseudo code of wrapping Model.train_step() is as follows. We wrap .train_step() into a new function, train_function() , convert it to a tf.function , and return it to .fit() . In train_function() , we just call .train_step() using distribute.Strategy.run() and aggregate the outputs and return it. class Model ( Layer ): def make_train_function ( self , ... ): def train_function ( iterator ): data = next ( iterator ) outputs = model . distribute_strategy . run ( self . train_step , args = ( data ,)) outputs = reduce_per_replica ( outputs ) return outputs train_function = tf . function ( train_function ) return train_function There is another TensorFlow distribute strategy API that is used by Keras is tf.distribute.get_strategy() . That is how Keras get the distribute strategy defined by the user. TensorFlow API tf.distribute.get_strategy() Returns the current tf.distribute.Strategy object.","title":"Distributed training"},{"location":"09-the-predict-function/","text":"( Source ) The logic of the Model.predict() function is very similar to Model.fit() as shown in the following pseudo code. It first wraps the input x into a DataHandler . Then, build the Model.predict_step() into a tf.function with Model.make_predict_function() . .predict_step() make predicitons for a single batch of data, which can also be overridden to customize the predict behavior. Similar to .make_train_function() , .make_predict_function() would handle the distribute strategy while building the tf.function . class Model ( Layer ): def predict ( self , x , ... ): data_handler = DataHandler ( x ) self . predict_function = self . make_predict_function () outputs = [] for epoch , iterator in data_handler . enumerate_epochs (): for step in data_handler . steps (): outputs . append ( self . predict_function ( iterator )) return outputs By default, the Model.predict_step() function would just unpack x from the provided data (because data may contain y ) and call the model to do a forward pass with the data as shown in the following pseudo code. ( Source ) class Model ( Layer ): def predict_step ( self , data ): x = data_adapter . unpack ( data ) return self ( x , training = False )","title":"9. The predict function"},{"location":"10-the-evaluate-function/","text":"( Source ) The logic of the Model.evaluate() function is very similar to Model.fit() and Model.predict() as shown in the following pseudo code. It first wraps the input x and y into a DataHandler . Then, build the Model.test_step() into a tf.function with Model.make_test_function() . .test_step() do evaluations for a single batch of data, which can also be overridden to customize its behavior. Similar to .make_train_function() and .make_predict_function() , .make_test_function() would handle the distribute strategy while building the tf.function . class Model ( Layer ): def evaluate ( self , x , y , ... ): data_handler = DataHandler ( x , y ) self . test_function = self . make_test_function () logs = {} for epoch , iterator in data_handler . enumerate_epochs (): for step in data_handler . steps (): # Always record the last epoch's metric. logs = self . test_function ( iterator ) return logs By default, the Model.test_step() function would just unpack x and y from the provided data and call the model to do a forward pass with the data to get the predictions. Then, it uses the prediction and the ground truth y to compute the metric values to return. The pseudo code is shown as follows. ( Source ) class Model ( Layer ): def predict_step ( self , data ): x = data_adapter . unpack ( data ) y_pred = self ( x , training = False ) self . compiled_metrics . update_state ( y , y_pred ) return_metrics = {} for metric in self . metrics : return_metrics [ metric . name ] = metric . result () return return_metrics","title":"10. The evaluate function"},{"location":"11-the-losses/","text":"( Source ) All loss functions implemented in Keras are subclasses of the Loss class. For example, you can implement a MeanSquaredError loss as follows. class MeanSquaredError ( Loss ): def call ( self , y_true , y_pred ): return tf . reduce_mean ( tf . math . square ( y_pred - y_true ), axis =- 1 ) You can use it as a standalone function as follows. Loss.call() is called by Loss.__call__() under the hood. loss = MeanSquaredError () print ( loss ( np . array ([ 0.0 , 1.0 ]), np . array ([ 1.0 , 1.0 ]))) # tf.Tensor(0.5, shape=(), dtype=float64) All the built-in losses are implemented in a similar way, which is to override the call() function. It computes the loss for the given ground truth and predictions. As you may know, the loss passed to .compile() can be a string, a function, or a Loss subclass instance. However, they are all converted to a Loss subclass in the end. In .comiple() , it uses keras.losses.get() to convert the losses. The get() function can accept a string, which is the name of the loss, and returns either a loss function or a Loss subclass instance. If it is a function, Keras would further convert it to a Loss subclass instance using the LossFunctionWrapper , which wraps the function into a Loss subclass instance. The overall converting process is shown in the following pseudo-code. ( Source ) def get_loss ( loss ): loss = keras . losses . get ( loss ) # If it is an function if not isinstance ( loss , keras . losses . Loss ): loss = losses_mod . LossFunctionWrapper ( loss , name = loss_name ) return loss The LossFunctionWrapper class is just a subclass of Loss and calls the provided loss function in its .call() function as shown in the following pseudo-code. We also show an example of wrapping up a mean_squared_error() loss function into a Loss subclass instance. class LossFunctionWrapper ( Loss ): def __init__ ( self , fn ): self . fn = fn def call ( self , y_true , y_pred ): return self . fn ( y_true , y_pred ) def mean_squared_error ( y_true , y_pred ): return tf . reduce_mean ( tf . math . square ( y_pred - y_true ), axis =- 1 ) loss = LossFunctionWrapper ( mean_squared_error ) It is a common pattern in Keras, which wraps a function into a class instance. We will see this pattern again in metrics as well, which will be introduced later. As you can see from above, the Loss.call() function is only dealing with a single batch of data. During training, computing the loss of a single batch of data is enough for backpropagation. However, we also need to compute the average loss value of all the trained batches to print to screen during training. It is done by the LossesContainer class, which we mentioned when introducing .compile() . It manages a metric to track the historical loss values for the batches. Unlike a loss function, the metric can not only compute the metric for one batch of data but also record some statistics across the historical batches. We will introduce more details about how metrics work in Keras later. The LossesContainer class also manages multiple losses, which is supported in Keras. The user can pass multiple losses to a model with multiple heads, where each loss corresponds to one head. The LossesContainer class contains all these losses and exposes methods that can manage them as if they are a single object. This is also a pattern. Using containers to manage a collection of objects and behaves similarly to a single object. We will see it again when we introduce the metrics. The only method of LossesContainer that directly called by the Model class is .__call__() . As we introduced in .fit() part, .train_step() use self.compiled_loss(y, y_pred) to compute the loss value. In LossesContainer.__call__() , it iterates through the different heads of the model and computes the losses, and sums them up. The pseudo-code is as follows. ( Source ) class LossesContainer ( Container ): def __call__ ( self , y_true , y_pred ): # y_pred is a list of outputs. # Each element in the list is the output of one of the heads. # y_true is the ground truth for the heads in a similar format. for single_y_true , single_y_pred , single_loss in zip ( y_true , y_pred , self . _losses ): loss_values . append ( single_loss ( single_y_true , single_y_pred )) total_loss = sum ( loss_values ) # Updating the metric tracking the average loss value # across the historical batches. self . _losses_metric . update_state ( total_loss ) return total_loss","title":"11. The losses"},{"location":"12-the-metrics/","text":"( Source ) All metrics in Keras are derived from the Metric class. A metric can be used directly instead of passing it to a model as shown in the following code example. We call the .update_state() function multiple times to pass the y_true and y_pred in different batches to it. Then, use .result() to get the metric value. We can also use .reset_state() to clear all previous computed values. mse = keras . metrics . MeanSquaredError () mse . update_state ([[ 0 , 1 ], [ 0 , 0 ]], [[ 1 , 1 ], [ 0 , 0 ]]) mse . update_state ([[ 0 , 1 ], [ 0 , 0 ]], [[ 1 , 1 ], [ 0 , 0 ]]) print ( mse . result () . numpy ()) # 0.25 mse . reset_state () mse . update_state ([[ 0 , 1 ]], [[ 1 , 1 ]]) print ( mse . result () . numpy ()) # 0.5 When subclassing the Metric class, one needs to override .update_state() , .result() , and .reset_state() . Refer to this tutorial for more details on how to implement a custom metric. To keep track of all the statistics in the metric, which are all tf.Variable s, Metric extends the Layer class. The .update_state() is compiled into a tf.function for faster computation. The pseudo-code of the Metric class is shown as follows. ( Source ) class Metric ( Layer ): def __new__ ( self , cls , * args , ** kwargs ): obj = super ( Metric , cls ) . __new__ ( cls ) obj . update_state = tf . function ( obj . update_state ) return obj def update_state ( self ): raise NotImplementedError def result ( self ): raise NotImplementedError def reset_state ( self ): raise NotImplementedError Take notes of the following subclasses of Metric . Reduce is a metric that computes a single value out of a tensor. The computation is defined by an argument passed to the initializer. For example, it can be computing the sum or mean of all the values in the tensor. Mean is a subclass of Reduce , which just computes the mean of the values in the tensor. MeanMetricWrapper is a subclass of Mean . It is similar to LossFunctionWrapper introduced in the previous section. It converts a metric function into a Metric subclass. It extends Mean because all metric functions need to be averaged across batches. In Model.compile() , all the metrics are wrapped up into a single MetricsContainer instance. Similar to the LossesContainer , it encapsulates all the metrics to be easily used by the Model class. It implements .update_state() and .reset_state() just like a Metric subclass so that the Model class will use this MetricsContainer just like a single metric. It doesn't need to implement .result() because the result of each metric is displayed separately. During initialization, it converts the metric strings or functions to Metric subclass instances. Notably, the metrics MetricsContainer receives is a list of lists of metrics because each head of the neural network model has a list of metrics. The pseudo-code of MetricsContainer is shown as follows. ( Source ) class MetricsContainer ( Container ): def __init__ ( self , metrics ): self . _metrics = [ self . _get_metric_object ( metric ) for metric in metrics ] def update_state ( self , y_true , y_pred ): # y_pred is a list of outputs. # Each element in the list is the output of one of the heads. # y_true is the ground truth for the heads in a similar format. for single_y_true , single_y_pred , single_metrics in zip ( y_true , y_pred , self . _metrics ): # Iterate the metrics for the current head. for metric_obj in single_metrics : metric_obj . update_state ( single_y_true , single_y_pred ) def reset_state ( self ): for metric_obj in tf . nest . flatten ( self . _metrics ): metric_obj . reset_state () def _get_metric_objects ( self , metric ): # get() may return a function instead of a Metric instance. metric_obj = keras . metrics . get ( metric ) if not isinstance ( metric , keras . metrics . Metric ): metric_obj = keras . metrics . MeanMetricWrapper ( metric_obj ) return metric_obj","title":"12. The metrics"},{"location":"13-the-optimizers/","text":"( Source ) In Model.train_step() , which we introduced in previous chapters, optimizer.minimize() is called directly to update the trainable variables to reduce the loss function value, while the gradients are recorded in the tape . The pseudo-code is shown below. class Model ( Layer ): def train_step ( self , data ): x , y = data_adapter . unpack ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) loss = self . compiled_loss ( y , y_pred ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . compiled_metrics . update_state ( y , y_pred ) return { metric . name : metric . result () for metric in self . metrics } To understand how this optimizer works, let's see what happens behind the .minimize() function. All optimizers in Keras extends the OptimizerV2 class, which extends the Tractable class. Remember that the Layer class also extends the Tractable class. They all have variables to track. Any tf.Variable in its attributes will be tracked automatically by TensorFlow. In OptimizerV2.minimize() , which calls another method, OptimizerV2.apply_gradients() to update the gradients. In .minimize() the gradients are either obtained from the gradient tape, which is passed to it as an argument or computed in the function using loss and var_list , which is a list of trainable variables passed to it. When calling .apply_gradients() , we zip the gradients and their corresponding variables into pairs and pass them to it. In .apply_gradients() , it updates the variables distributedly. The internal function of update_var() is executed distributedly, which calls ._resource_apply_dense() , which is a function for the subclasses to override to update the variable values with the gradients. ( Source ) class OptimizerV2 ( Trackable ): def minimize ( self , loss , var_list , tape = None ): tape = tape if tape is not None else tf . GradientTape () with tape : grads = tape . gradient ( loss , var_list ) self . apply_gradients ( zip ( grads , var_list )) def apply_gradients ( grads_and_vars ): def update_var ( var , grad ): return self . _resource_apply_dense ( grad , var ) strategy = tf . distribute . get_strategy () for grad , var in grads_and_vars : with strategy . extended . colocate_vars_with ( var ): distribution . extended . update ( var , update_var , args = ( grad , )) def _resource_apply_dense ( self , grad , var ): raise NotImplementedError TensorFlow API tf.GradientTape() Besides recording the tape using a with statement, it can also be used in a stand-alone mode to return the gradient tape that automatically recorded the gradients during the forward-pass by TensorFlow. TensorFlow API tf.distribute.StrategyExtended The strategy.extended in the code example above is actually an instance of StrategyExtended . All distribute strategies in TensorFlow have a .extended attribute. It exposes some device and locality control of the variables and tensors. For example, .colocate_vars_with(var) opens a scope where all the newly created variables would be on the same device as var . .update(var, update_var, args=(grad, )) run update_var to update var by mirroring the args to the same device. To make your own optimizer, you may need to override some of the functions, for example, ._resource_apply_dense() . Here is the pseudo-code for implementing a stochastic gradient descent optimizer. We just override ._resource_apply_dense() and call the corresponding TensorFlow operation to update the variables. ( Source ) class SGD ( OptimizerV2 ): def _resource_apply_dense ( self , grad , var ): tf . raw_ops . ResourceApplyGradientDescent ( var = var . handle , delta = grad ) Note that there are other functions to override to deal with other types of tensors, for example, the sparse tensors. TensorFlow API tf.raw_ops This raw_ops module in TensorFlow is a collection of raw C++ TensorFlow ops for the user to directly use in Python. Each op is a series of tensor operations that corresponds to a GPU kernel implemented in TensorFlow. Please refer to this guide for more details about a TensorFlow op. It shows how to create a custom op.","title":"13. The optimizers"}]}